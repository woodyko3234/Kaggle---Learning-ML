{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to deal with NaN values?\n",
    "https://www.kaggle.com/dansbecker/handling-missing-values/code\n",
    "\n",
    "### 1) A Simple Option: Drop Columns with Missing Values\n",
    "\n",
    "### 2) A Better Option: Imputation\n",
    "Imputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n",
    "\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    my_imputer = Imputer()\n",
    "    data_with_imputed_values = my_imputer.fit_transform(original_data)\n",
    "\n",
    "The default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n",
    "\n",
    "One (of many) nice things about Imputation is that it can be included in a scikit-learn Pipeline. Pipelines simplify model building, model validation and model deployment.\n",
    "\n",
    "### 3) An Extension To Imputation\n",
    "\n",
    "Imputation is the standard approach, and it usually works well. However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. Here's how it might look:\n",
    "\n",
    "    # make copy to avoid changing original data (when Imputing)\n",
    "    new_data = original_data.copy()\n",
    "\n",
    "    # make new columns indicating what will be imputed\n",
    "    cols_with_missing = (col for col in new_data.columns \n",
    "                                 if new_data[c].isnull().any())\n",
    "    for col in cols_with_missing:\n",
    "        new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "    # Imputation\n",
    "    my_imputer = Imputer()\n",
    "    new_data = my_imputer.fit_transform(new_data)\n",
    "    In some cases this approach will meaningfully improve results. In other cases, it doesn't help at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "melb_data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "melb_target = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
    "\n",
    "# For the sake of keeping the example simple, we'll use only numeric predictors. \n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from dropping columns with Missing Values:\n",
      "346141.16164160176\n"
     ]
    }
   ],
   "source": [
    "cols_with_missing = [col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n",
    "print(\"Mean Absolute Error from dropping columns with Missing Values:\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation:\n",
      "206433.87735096936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print(\"Mean Absolute Error from Imputation:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation while Track What Was Imputed:\n",
      "204247.1418554086\n"
     ]
    }
   ],
   "source": [
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "cols_with_missing = (col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Drop houses where the target is missing\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# Since missing values isn't the focus of this tutorial, we use the simplest\n",
    "# possible approach, which drops these columns. \n",
    "# For more detail (and a better approach) to missing values, see\n",
    "# https://www.kaggle.com/dansbecker/handling-missing-values\n",
    "cols_with_missing = [col for col in train_data.columns \n",
    "                                 if train_data[col].isnull().any()]                                  \n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n",
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# We use it as our only way to select categorical columns here. This is convenient, though\n",
    "# a little arbitrary.\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiscVal           int64\n",
       "RoofStyle        object\n",
       "OpenPorchSF       int64\n",
       "Street           object\n",
       "BsmtHalfBath      int64\n",
       "Utilities        object\n",
       "BsmtFullBath      int64\n",
       "Heating          object\n",
       "LotShape         object\n",
       "EnclosedPorch     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictors.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 18521\n",
      "Mean Absolute Error with One-Hot Encoding: 18263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_mae(X, y):\n",
    "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50), \n",
    "                                X, y, \n",
    "                                scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Absolute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n",
    "                                                                    join='left', \n",
    "                                                                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "data = pd.read_csv('train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KunWuYao/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(train_X, train_y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 17824.178424657533\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "predictions = my_model.predict(test_X)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators and early_stopping_rounds\n",
    "n_estimators specifies how many times to go through the modeling cycle described above.\n",
    "\n",
    "In the underfitting vs overfitting graph, n_estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal. Typical values range from 100-1000, though this depends a lot on the learning rate discussed below.\n",
    "\n",
    "The argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "\n",
    "Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores.\n",
    "\n",
    "When using early_stopping_rounds, you need to set aside some of your data for checking the number of rounds to use. If you later want to fit a model with all of your data, set n_estimators to whatever value you found to be optimal when run with early stopping.\n",
    "\n",
    "#### learning_rate\n",
    "Here's a subtle but important trick for better XGBoost models:\n",
    "\n",
    "Instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n",
    "\n",
    "So, you can use a higher value of n_estimators without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.\n",
    "\n",
    "In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n",
    "\n",
    "#### n_jobs\n",
    "On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command.\n",
    "\n",
    "XGBoost has a multitude of other parameters, but these will go a very long way in helping you fine-tune your XGBoost model for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
    "             eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Partial Dependence Plots\n",
    "Some people complain machine learning models are black boxes. These people will argue we cannot see how these models are working on any given dataset, so we can neither extract insight nor identify problems with the model.\n",
    "\n",
    "By and large, people making this claim are unfamiliar with partial dependence plots. Partial dependence plots show how each variable or predictor affects the model's predictions. This is useful for questions like:\n",
    "\n",
    "How much of wage differences between men and women are due solely to gender, as opposed to differences in education backgrounds or work experience?\n",
    "\n",
    "Controlling for house characteristics, what impact do longitude and latitude have on home prices? To restate this, we want to understand how similarly sized houses would be priced in different areas, even if the homes actually at these sites are different sizes.\n",
    "\n",
    "Are health differences between two groups due to differences in their diets, or due to other factors?\n",
    "\n",
    "If you are familiar with linear or logistic regression models, partial dependence plots can be interepreted similarly to the coefficients in those models. But partial dependence plots can capture more complex patterns from your data, and they can be used with any model. If you aren't familiar with linear or logistic regressions, don't get caught up on that comparison.\n",
    "\n",
    "We will show a couple examples below, explain what they mean, and then talk about the code.\n",
    "\n",
    "Interpreting Partial Dependence Plots\n",
    "We'll start with 2 partial dependence plots showing the relationship (according to our model) between Price and a couple variables from the Melbourne Housing dataset. We'll walk through how these plots are created and interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAADPCAYAAABr76FoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxLUVI77PjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1151076d8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "cols_to_use = ['Distance', 'Landsize', 'BuildingArea']\n",
    "\n",
    "def get_some_data():\n",
    "    data = pd.read_csv('melb_data.csv')\n",
    "    y = data.Price\n",
    "    X = data[cols_to_use]\n",
    "    my_imputer = Imputer()\n",
    "    imputed_X = my_imputer.fit_transform(X)\n",
    "    return imputed_X, y\n",
    "    \n",
    "\n",
    "X, y = get_some_data()\n",
    "my_model = GradientBoostingRegressor()\n",
    "my_model.fit(X, y)\n",
    "fig, axs = plot_partial_dependence(my_model, \n",
    "                                   features=[0,2], \n",
    "                                   X=X, \n",
    "                                   feature_names=cols_to_use, \n",
    "                                   grid_resolution=10)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot shows the partial dependence between our target, Sales Price, and the distance variable. Distance in this dataset measures the distance to Melbourne's central business district.\n",
    "\n",
    "**The partial dependence plot is calculated only after the model has been fit**. The model is fit on real data. In that real data, houses in different parts of town may differ in myriad ways (different ages, sizes, etc.) But after the model is fit, we could start by taking all the characteristics of a single house. Say, a house with 2 bedrooms, 2 bathrooms, a large lot, an age of 10 years, etc.\n",
    "\n",
    "We then use the model to predict the price of that house, but we change the distance variable before making a prediction. We first predict the price for that house when sitting distance to 4. We then predict it's price setting distance to 5. Then predict again for 6. And so on. We trace out how predicted price changes (on the vertical axis) as we move from small values of distance to large values (on the horizontal axis).\n",
    "\n",
    "In this description, we used only a single house. But because of interactions, the partial dependence plot for a single house may be atypical. So, instead we repeat that mental experiment with multiple houses, and we plot the average predicted price on the vertical axis. You'll see some negative numbers. That doesn't mean the price would sell for a negative price. Instead it means the prices would have been less than the actual average price for that distance.\n",
    "\n",
    "In the left graph, we see house prices fall as we get further from the central business distract. Though there seems to be a nice suburb about 16 kilometers out, where home prices are higher than many nearer and further suburbs.\n",
    "\n",
    "The right graph shows the impact of building area, which is interpreted similarly. A larger building area means higher prices.\n",
    "\n",
    "These plots are useful both to extract insights, as well as to sanity check that your model is learning something you think is sensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tips related to plot_partial_dependence:\n",
    "\n",
    "The features are the column numbers from the X array or dataframe that you wish to have plotted. This starts to look bad beyond 2 or 3 variables. You could make repeated calls to plot 2 or 3 at a time.\n",
    "\n",
    "There are options to establish what points on the horizontal axis are plotted. The simplest is grid_resolution which we use to determine how many different points are plotted. These plots tend to look jagged as that value increases, because you will pick up lots of randomness or noise in your model. It's best not to take the small or jagged fluctuations too literally. Smaller values of grid_resolution smooth this out. It's also much less of an issue for datasets with many rows.\n",
    "\n",
    "There is a function called partial_dependence to get the raw data making up this plot, rather than making the visual plot itself. This is useful if you want to control how it is visualized using a plotting package like Seaborn. With moderate effort, you could make much nicer looking plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Pipelines\n",
    "Pipelines are a simple way to keep your data processing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "\n",
    "Many data scientists hack together models without pipelines, but Pipelines have some important benefits. Those include:\n",
    "\n",
    "    Cleaner Code: You won't need to keep track of your training (and validation) data at each step of processing. Accounting for data at each step of processing can get messy. With a pipeline, you don't need to manually keep track of each step.\n",
    "    Fewer Bugs: There are fewer opportunities to mis-apply a step or forget a pre-processing step.\n",
    "    Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "    More Options For Model Testing: You will see an example in the next tutorial, which covers cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read Data\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_pipeline.fit(train_X, train_y)\n",
    "predictions = my_pipeline.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_imputer = Imputer()\n",
    "my_model = RandomForestRegressor()\n",
    "\n",
    "imputed_train_X = my_imputer.fit_transform(train_X)\n",
    "imputed_test_X = my_imputer.transform(test_X)\n",
    "my_model.fit(imputed_train_X, train_y)\n",
    "predictions = my_model.predict(imputed_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pipelines\n",
    "Most scikit-learn objects are either transformers or models.\n",
    "\n",
    "Transformers are for pre-processing before modeling. The Imputer class (for filling in missing values) is an example of a transformer. Over time, you will learn many more transformers, and you will frequently use multiple transformers sequentially.\n",
    "\n",
    "Models are used to make predictions. You will usually preprocess your data (with transformers) before putting it in a model.\n",
    "\n",
    "You can tell if an object is a transformer or a model by how you apply it. After fitting a transformer, you apply it with the transform command. After fitting a model, you apply it with the predict command. Your pipeline must start with transformer steps and end with a model. This is what you'd want anyway.\n",
    "\n",
    "Eventually you will want to apply more transformers and combine them more flexibly. We will cover this later in an Advanced Pipelines tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation: \n",
    "https://www.kaggle.com/dansbecker/cross-validation/code\n",
    "\n",
    "### Trade-offs Between Cross-Validation and Train-Test Split\n",
    "Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take more time to run, because it estimates models once for each fold. So it is doing more total work.\n",
    "\n",
    "Given these tradeoffs, when should you use each approach? On small datasets, the extra computational burden of running cross-validation isn't a big deal. These are also the problems where model quality scores would be least reliable with train-test split. So, if your dataset is smaller, you should run cross-validation.\n",
    "\n",
    "For the same reasons, a simple train-test split is sufficient for larger datasets. It will run faster, and you may have enough data that there's little need to re-use some of it for holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-325747.40698985 -309042.23194956 -278166.09490396]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 304318.577948\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error %2f' %(-1 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data leakage: https://www.kaggle.com/dansbecker/data-leakage/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
